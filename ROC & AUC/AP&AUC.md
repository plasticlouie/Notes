# 分类器评价指标AP和AUC

## AP和AUC

**AP:**
Recall-Precision曲线的积分

```
Recall    = TP/P
Precision = TP/(TP+FP)
```

**AUC:**
ROC(FPR-TPR)曲线的积分
```
FPR = FP/N
TPR = TP/P
```

## 样本不平衡问题

假设一个分类器在样本平衡的情况下，选取某个score的阈值可以得到如下所示的performance。
```
   TP      FN
[--------|----]  P
   FN      FP
[--------|----]  N
```
现在固定分类器的参数不动，把负样本复制若干倍。
```
   TP      FN
[--------|----]  P
           FN                  FP
[------------------------|------------]  N
```
1. ROC曲线中的```TPR```和```FPR```都保持不变。
2. PR曲线中的```Recall```不变，而```Precision=TP/(TP+FP)```下降了很多。

**结论：**
在保持分类器不变的情况下，如果增加大量负样本（符合原负样本的分布规律），ROC曲线受到的影响很小（不敏感），而PR曲线会受到很大的影响（敏感）。

## 样本不平衡情况下的适用条件

ROC曲线反映的是
**把一定比例的负样本错分为正样本**
的情况下，
**多少正样本被正确分类**
。在负样本数量很多的情况下对FP比较不敏感。

举个栗子，假设10000个人中有10个人患病，用算法1诊断找出了其中9个病例，但是把正常人(9990)中的10个误认为患者。用算法2也找出了9个病例，但把正常人中的20个误认为患者。
```
算法1
  TP  FN
[----|-]
              FN              FP
[----------------------------|-]

算法2
  TP  FN
[----|-]
              FN             FP
[---------------------------|--]
```

算法1和算法2对比，```TPR```都是```0.9```，```FPR```为```10/10000```和```20/10000```，相差（两者相减的绝对值）很小。但两者的```Precision```为```9/10```和```9/20```，差别很大。

ROC曲线适用于评估比较关注于```TPR```，而对于分类器内部的准确率不敏感的情况。比如说疾病诊断，目标是尽量不要漏诊，算法1和算法2都能找出90%的患者，虽然两者的```Precision```相差很大，但把正常人误诊为患者的比例占总人群的比例(```FPR```)很小（0.1%-0.2%），所以认为两者的表现都很好。用AUC值来衡量的话，两个算法差别不大。

而PR曲线关注的是
**在保持一定Recall**
的情况下
**分类器认为是正例的样本中有多少是正确的**
。相比于ROC曲线更关注分类器内部的准确性。

举个栗子，有10000支股票，其中10支可以盈利。算法1找出了9支盈利的股票，但是把亏损股票(9990)中的10个错分为盈利的，算法2找出了9支盈利股票，但把亏损股票中的20个认为可盈利。两者的```Precision```分别为```9/19(0.47)```和```9/29(0.31)```，差别很大，算法2要赔死了。对股票预测这个case来说，衡量某个分类器的性能用PR曲线比用ROC曲线更适合。
```
算法1
  TP  FN
[----|-]
              FN              FP
[----------------------------|-]

算法2
  TP  FN
[----|-]
              FN             FP
[---------------------------|--]
```
